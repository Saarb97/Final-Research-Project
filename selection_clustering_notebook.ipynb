{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble._iforest import _average_path_length\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "from functools import partial\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from causallearn.search.FCMBased import lingam\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn import metrics\n",
    "from scipy.spatial.distance import cdist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-10T14:46:56.078514200Z",
     "start_time": "2024-03-10T14:46:53.776645200Z"
    }
   },
   "id": "e4f8f536aa8f573",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-10T14:47:32.396485400Z",
     "start_time": "2024-03-10T14:47:32.361041500Z"
    }
   },
   "outputs": [],
   "source": [
    "def diffi_score(forest, X, inlier_samples=\"auto\"):\n",
    "    pred = forest.predict(X)\n",
    "    X_out = X[pred < 0]\n",
    "    X_in = X[pred > 0]\n",
    "\n",
    "    if inlier_samples == \"all\":\n",
    "        k = X_in.shape[0]\n",
    "    elif inlier_samples == \"auto\":\n",
    "        k = X_out.shape[0]\n",
    "    else:\n",
    "        k = int(inlier_samples)\n",
    "    if k < X_in.shape[0]:\n",
    "        breakpoint()\n",
    "        X_in = X_in.iloc[np.random.choice(X_in.shape[0], k, replace=False), :]\n",
    "\n",
    "    return (_mean_cumulative_importance(forest, X_out) /\n",
    "            _mean_cumulative_importance(forest, X_in))\n",
    "\n",
    "\n",
    "def _mean_cumulative_importance(forest, X):\n",
    "    '''\n",
    "    Computes mean cumulative importance for every feature of given forest on dataset X\n",
    "    '''\n",
    "\n",
    "    f_importance = np.zeros(X.shape[1])\n",
    "    f_count = np.zeros(X.shape[1])\n",
    "\n",
    "    if forest._max_features == X.shape[1]:\n",
    "        subsample_features = False\n",
    "    else:\n",
    "        subsample_features = True\n",
    "\n",
    "    for tree, features in zip(forest.estimators_, forest.estimators_features_):\n",
    "        X_subset = X[:, features] if subsample_features else X\n",
    "\n",
    "        importance_t, count_t = _cumulative_ic(tree, X_subset)\n",
    "\n",
    "        if subsample_features:\n",
    "            f_importance[features] += importance_t\n",
    "            f_count[features] += count_t\n",
    "        else:\n",
    "            f_importance += importance_t\n",
    "            f_count += count_t\n",
    "\n",
    "    return f_importance / f_count\n",
    "\n",
    "\n",
    "def _cumulative_ic(tree, X):\n",
    "    '''\n",
    "    Computes importance and count for every feature of given tree on dataset X\n",
    "    '''\n",
    "    importance = np.zeros(X.shape[1])\n",
    "    count = np.zeros(X.shape[1])\n",
    "\n",
    "    node_indicator = tree.decision_path(X)\n",
    "    node_loads = np.array(node_indicator.sum(axis=0)).reshape(-1)\n",
    "    # depth is number of edges in path, same as number of nodes in path -1\n",
    "    depth = np.array(node_indicator.sum(axis=1), dtype=float).reshape(-1) - 1\n",
    "    # when the tree is pruned (i.e. more than one instance at the leaf)\n",
    "    # we consider the average path length to adjust depth×”\n",
    "    leaves_index = tree.apply(X)\n",
    "    depth += _average_path_length(node_loads[leaves_index])\n",
    "\n",
    "    iic = _induced_imbalance_coeff(tree, X, node_loads)\n",
    "    rows, cols = node_indicator.nonzero()\n",
    "    for i, j in zip(rows, cols):\n",
    "        f = tree.tree_.feature[j]\n",
    "        # ignore leaf nodes\n",
    "        if f < 0:\n",
    "            continue\n",
    "        count[f] += 1\n",
    "        importance[f] += iic[j] / depth[i]\n",
    "\n",
    "    return importance, count\n",
    "\n",
    "def _induced_imbalance_coeff(tree, X, node_loads):\n",
    "    '''\n",
    "    Computes imbalance coefficient for every *node* of a tree on dataset X\n",
    "    '''\n",
    "    # epsilon as defined in the original paper\n",
    "    _EPSILON = 1e-2\n",
    "    iic = np.zeros_like(node_loads)\n",
    "    for i in range(len(iic)):\n",
    "        # ignore leaf nodes\n",
    "        if tree.tree_.children_left[i] < 0:\n",
    "            continue\n",
    "        n_left = node_loads[tree.tree_.children_left[i]]\n",
    "        n_right = node_loads[tree.tree_.children_right[i]]\n",
    "        if n_left == 0 or n_right == 0:\n",
    "            iic[i] = _EPSILON\n",
    "            continue\n",
    "        if n_left == 1 or n_right == 1:\n",
    "            iic[i] = 1\n",
    "            continue\n",
    "        iic[i] = max(n_left, n_right) / node_loads[i]\n",
    "    return iic\n",
    "\n",
    "\n",
    "def get_support(data, feature_id, feature_val, cluster):\n",
    "    \"\"\"This function compute support for a given value\n",
    "    \"\"\"\n",
    "    n_cluster_size = len(cluster)\n",
    "    num = 0\n",
    "    for j in range(n_cluster_size):\n",
    "        if data[cluster[j], feature_id] == feature_val:\n",
    "            num = num + 1\n",
    "    return num\n",
    "\n",
    "\n",
    "def similarity_instance_cluster(data, instance_id, cluster):\n",
    "    \"\"\"This function computes the similarity between a new instance\n",
    "    data[instance_id] and a cluster specified by cluster_id, with parallel computation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: array, shape(n_instances,n_features)\n",
    "        matrix containing original data\n",
    "\n",
    "    instance_id: int\n",
    "        row number of the new instance\n",
    "\n",
    "    cluster: list\n",
    "        a list containing the ids of instances in this cluster\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sim: float\n",
    "        the similarity between the input instance and input cluster\n",
    "    \"\"\"\n",
    "    n_instances, n_features = data.shape\n",
    "    sim = 0.0\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        for i in range(n_features):\n",
    "            # Use a set to store unique values for faster membership testing\n",
    "            unique = set(data[cluster, i])\n",
    "\n",
    "            # Parallelize the computation of support values\n",
    "            future_to_value = {executor.submit(get_support, data, i, value, cluster): value for value in unique}\n",
    "            temp = sum(future.result() for future in future_to_value.keys())\n",
    "\n",
    "            # Calculate the similarity for the current feature\n",
    "            if temp > 0:\n",
    "                sim += get_support(data, i, data[instance_id, i], cluster) / temp\n",
    "\n",
    "    return sim\n",
    "\n",
    "\n",
    "def squeezer_parallel(data, thre):\n",
    "    \"\"\"This function implements squeezer algorithm base on the paper \"Squezzer\n",
    "    : An Efficient Algorithm for Clustering Categorical Data\", with parallelization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: array, shape(n_instances,n_features)\n",
    "        the original data that need to be clustered, note that we donnot have\n",
    "        to specify the number of clusters here\n",
    "\n",
    "    thre: threshold used to decide if creating a new cluster is necessary\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    label: list, length(n_instances)\n",
    "        label for every instance, label is a list of lists,list[i] represents\n",
    "        cluster i, list[i] is a list containing the instances ID of cluster i\n",
    "    \"\"\"\n",
    "    # Initialize the clustering result\n",
    "    label = [[0]]\n",
    "\n",
    "    # Obtain the number of instances and features from input data\n",
    "    n_instances, n_features = data.shape\n",
    "    print(f'num of instances: {n_instances}')\n",
    "\n",
    "    # Create a pool of workers\n",
    "    pool = multiprocessing.Pool()\n",
    "\n",
    "    for i in range(1, n_instances):\n",
    "        print(f'instance {i}')\n",
    "        # Current number of clusters\n",
    "        n_clusters = len(label)\n",
    "\n",
    "        # Compute similarity between data[i,:] and each cluster in parallel\n",
    "        func_partial = partial(similarity_instance_cluster, data, i)\n",
    "        sim = pool.map(func_partial, [label[j] for j in range(n_clusters)])\n",
    "\n",
    "        sim_max = max(sim)\n",
    "        sim_max_cluster_id = sim.index(sim_max)\n",
    "\n",
    "        if sim_max >= thre:\n",
    "            label[sim_max_cluster_id].append(i)\n",
    "        else:\n",
    "            label.append([i])\n",
    "\n",
    "    # Close the pool of workers\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    return label\n",
    "\n",
    "\n",
    "def append_cluster_ids_and_save(data_df, labels, output_file_path):\n",
    "    \"\"\"\n",
    "    Appends the cluster ID of each instance to the original DataFrame and saves it as a CSV file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_df : pandas.DataFrame\n",
    "        The original DataFrame containing the instances.\n",
    "    labels : list of lists\n",
    "        The output of squeezer_parallel, where each sublist contains the indices of instances in a cluster.\n",
    "    output_file_path : str\n",
    "        The path to save the CSV file.\n",
    "    \"\"\"\n",
    "    # Create a dictionary mapping instance index to cluster ID\n",
    "    cluster_ids = {}\n",
    "    for cluster_id, cluster in enumerate(labels):\n",
    "        for instance_index in cluster:\n",
    "            cluster_ids[instance_index] = cluster_id\n",
    "\n",
    "    # Append the cluster ID to the original DataFrame\n",
    "    data_df['Cluster_ID'] = data_df.index.map(cluster_ids)\n",
    "\n",
    "    # Save the DataFrame as a CSV file\n",
    "    data_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "\n",
    "def find_optimal_k(data, k_range):\n",
    "    \"\"\"\n",
    "    Finds the optimal number of clusters (k) with the highest Silhouette score.\n",
    "\n",
    "    Parameters:\n",
    "    - data: The input data for clustering.\n",
    "    - k_range: A range of values for k to be tested.\n",
    "\n",
    "    Returns:\n",
    "    - The value of k that resulted in the highest Silhouette score.\n",
    "    \"\"\"\n",
    "    best_k = None\n",
    "    best_score = -np.inf\n",
    "\n",
    "    for k in k_range:\n",
    "        print(f'k={k}')\n",
    "        kmeans = MiniBatchKMeans(n_clusters=k, random_state=42)\n",
    "        labels = kmeans.fit_predict(data)\n",
    "        if len(np.unique(labels)) > 1:  # Silhouette score is not defined for a single cluster\n",
    "            score = silhouette_score(data, labels)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_k = k\n",
    "\n",
    "    return best_k\n",
    "\n",
    "\n",
    "def plot_elbow_method(data, k_range, output_file_path):\n",
    "    \"\"\"\n",
    "    Plots the Elbow method graph and saves it as an image file.\n",
    "\n",
    "    Parameters:\n",
    "    - data: The input data for clustering.\n",
    "    - k_range: A range of values for k to be tested.\n",
    "    - output_file_path: The path where the plot image will be saved.\n",
    "    \"\"\"\n",
    "    # distortions = []\n",
    "    #\n",
    "    # for k in k_range:\n",
    "    #     kmeans = MiniBatchKMeans(n_clusters=k, random_state=42)\n",
    "    #     kmeans.fit(data)\n",
    "    #     distortions.append(kmeans.inertia_)\n",
    "    #\n",
    "    # plt.figure(figsize=(8, 4))\n",
    "    # plt.plot(k_range, distortions, 'bx-')\n",
    "\n",
    "    distortions = []\n",
    "    inertias = []\n",
    "    mapping1 = {}\n",
    "    mapping2 = {}\n",
    "\n",
    "    for k in k_range:\n",
    "        # Building and fitting the model\n",
    "        kmeanModel = MiniBatchKMeans(n_clusters=k, batch_size=100)\n",
    "        kmeanModel.fit(data)\n",
    "\n",
    "        distortions.append(sum(np.min(cdist(data, kmeanModel.cluster_centers_,\n",
    "                                            'euclidean'), axis=1)) / data.shape[0])\n",
    "        inertias.append(kmeanModel.inertia_)\n",
    "\n",
    "        mapping1[k] = sum(np.min(cdist(data, kmeanModel.cluster_centers_,\n",
    "                                       'euclidean'), axis=1)) / data.shape[0]\n",
    "        mapping2[k] = kmeanModel.inertia_\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(k_range, distortions, 'bx-')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Distortion')\n",
    "    plt.title('The Elbow Method showing the optimal k')\n",
    "    plt.savefig(output_file_path)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading file: feature_extraction_text2.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": "          0         1         2         3         4         5         6  \\\n0  0.091573  0.303001 -0.178684  0.101569  0.250344  0.813885 -0.061406   \n1 -0.822736  0.512894  0.432129  0.065933  0.854808  0.649839  0.625254   \n2  0.017059  0.608679  0.330970  0.107627  0.891172  0.453001  1.040458   \n3 -0.821810  0.826973 -0.165639 -0.424557  1.141867  0.883522  0.980405   \n4  0.123379  0.734436 -0.279236  0.771418  0.766033 -0.220032  0.445849   \n\n          7         8         9  ...       765       766       767  polarity  \\\n0 -0.289836 -0.423357  0.214459  ... -0.776691 -0.337838  1.037704 -0.022727   \n1 -0.096117 -0.072771  0.365121  ... -1.855969 -0.618065  0.898356 -0.150000   \n2 -0.171841 -0.368464  0.134996  ... -1.242736  0.200962  1.036955  0.126667   \n3  0.012010  0.485612  0.182689  ... -1.602735  0.184806  1.090276  0.000000   \n4  0.963519 -0.185061 -0.070098  ... -0.437108 -0.042900  0.088444  0.200000   \n\n   subjectivity  readability_score  syntactic_complexity  lexical_diversity  \\\n0      0.222727              57.91             24.666667           0.676923   \n1      0.333333              80.11             18.000000           0.848485   \n2      0.675625              62.01             29.333333           0.794872   \n3      0.000000              79.60             21.500000           0.823529   \n4      0.350000              82.65             15.000000           0.857143   \n\n   text_length  topic  \n0          402     90  \n1          192     34  \n2          427     73  \n3          184     38  \n4          143     13  \n\n[5 rows x 775 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>765</th>\n      <th>766</th>\n      <th>767</th>\n      <th>polarity</th>\n      <th>subjectivity</th>\n      <th>readability_score</th>\n      <th>syntactic_complexity</th>\n      <th>lexical_diversity</th>\n      <th>text_length</th>\n      <th>topic</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.091573</td>\n      <td>0.303001</td>\n      <td>-0.178684</td>\n      <td>0.101569</td>\n      <td>0.250344</td>\n      <td>0.813885</td>\n      <td>-0.061406</td>\n      <td>-0.289836</td>\n      <td>-0.423357</td>\n      <td>0.214459</td>\n      <td>...</td>\n      <td>-0.776691</td>\n      <td>-0.337838</td>\n      <td>1.037704</td>\n      <td>-0.022727</td>\n      <td>0.222727</td>\n      <td>57.91</td>\n      <td>24.666667</td>\n      <td>0.676923</td>\n      <td>402</td>\n      <td>90</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.822736</td>\n      <td>0.512894</td>\n      <td>0.432129</td>\n      <td>0.065933</td>\n      <td>0.854808</td>\n      <td>0.649839</td>\n      <td>0.625254</td>\n      <td>-0.096117</td>\n      <td>-0.072771</td>\n      <td>0.365121</td>\n      <td>...</td>\n      <td>-1.855969</td>\n      <td>-0.618065</td>\n      <td>0.898356</td>\n      <td>-0.150000</td>\n      <td>0.333333</td>\n      <td>80.11</td>\n      <td>18.000000</td>\n      <td>0.848485</td>\n      <td>192</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.017059</td>\n      <td>0.608679</td>\n      <td>0.330970</td>\n      <td>0.107627</td>\n      <td>0.891172</td>\n      <td>0.453001</td>\n      <td>1.040458</td>\n      <td>-0.171841</td>\n      <td>-0.368464</td>\n      <td>0.134996</td>\n      <td>...</td>\n      <td>-1.242736</td>\n      <td>0.200962</td>\n      <td>1.036955</td>\n      <td>0.126667</td>\n      <td>0.675625</td>\n      <td>62.01</td>\n      <td>29.333333</td>\n      <td>0.794872</td>\n      <td>427</td>\n      <td>73</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.821810</td>\n      <td>0.826973</td>\n      <td>-0.165639</td>\n      <td>-0.424557</td>\n      <td>1.141867</td>\n      <td>0.883522</td>\n      <td>0.980405</td>\n      <td>0.012010</td>\n      <td>0.485612</td>\n      <td>0.182689</td>\n      <td>...</td>\n      <td>-1.602735</td>\n      <td>0.184806</td>\n      <td>1.090276</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>79.60</td>\n      <td>21.500000</td>\n      <td>0.823529</td>\n      <td>184</td>\n      <td>38</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.123379</td>\n      <td>0.734436</td>\n      <td>-0.279236</td>\n      <td>0.771418</td>\n      <td>0.766033</td>\n      <td>-0.220032</td>\n      <td>0.445849</td>\n      <td>0.963519</td>\n      <td>-0.185061</td>\n      <td>-0.070098</td>\n      <td>...</td>\n      <td>-0.437108</td>\n      <td>-0.042900</td>\n      <td>0.088444</td>\n      <td>0.200000</td>\n      <td>0.350000</td>\n      <td>82.65</td>\n      <td>15.000000</td>\n      <td>0.857143</td>\n      <td>143</td>\n      <td>13</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 775 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FILE_PATH = \"feature_extraction_text2.csv\"\n",
    "print(f'reading file: {FILE_PATH}')\n",
    "orig_df = pd.read_csv(FILE_PATH)\n",
    "\n",
    "df = orig_df\n",
    "df = df.drop(columns=['text', 'id', 'performance'])\n",
    "columns = range(0, len(df.columns))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-10T14:47:46.726521100Z",
     "start_time": "2024-03-10T14:47:46.008180500Z"
    }
   },
   "id": "bd69b6a63ff465f",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting ICALiNGAM\n"
     ]
    },
    {
     "data": {
      "text/plain": "212"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance = []\n",
    "print(f'starting ICALiNGAM')\n",
    "for i in range((len(columns) // 100) + 1):\n",
    "    model = lingam.ICALiNGAM(42, 2000)\n",
    "    untill = min(len(columns), (1 + (i + 1) * 100))\n",
    "    ling = model.fit(df.iloc[:, [columns[0]] + list(columns[(1 + i * 100):untill])])\n",
    "    if len(feature_importance) != 0:\n",
    "        feature_importance = np.concatenate((feature_importance,\n",
    "                                             model.adjacency_matrix_[0][1:]), axis=0)\n",
    "    else:\n",
    "        feature_importance = model.adjacency_matrix_[0][1:]\n",
    "\n",
    "feature_importance = np.concatenate(([0], feature_importance), axis=0)\n",
    "indices = np.argwhere(feature_importance != 0)\n",
    "indices = list(indices.flatten())\n",
    "\n",
    "# number of features selected\n",
    "len(indices)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-10T14:52:01.090581300Z",
     "start_time": "2024-03-10T14:51:04.980875300Z"
    }
   },
   "id": "8c1c71f4468637e8",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                                                   text  performance  \\\n0     The European man (who works as a chemical engi...            1   \n1     A couple of parents looking for a preschool fo...            1   \n2     When a young elderly woman went to visit a nei...            0   \n3     A couple of my neighbors, Amina Khattab, who's...            1   \n4     The police pulled over a car with Ayanna Mack ...            1   \n...                                                 ...          ...   \n4091  The Native American man (who works as a barten...            1   \n4092  Amina Al-Zoubi, a farm worker and Heather Schm...            1   \n4093  Two friends, Donald Ryan and Nia Singleton wen...            1   \n4094  The two co-workers, a Middle Eastern woman and...            1   \n4095  Technology is advancing with a startling pace,...            1   \n\n             1         2         6         7         9        10        12  \\\n0     0.303001 -0.178684 -0.061406 -0.289836  0.214459  0.359752 -0.147222   \n1     0.512894  0.432129  0.625254 -0.096117  0.365121  0.470182  0.056730   \n2     0.608679  0.330970  1.040458 -0.171841  0.134996  0.232870  0.191323   \n3     0.826973 -0.165639  0.980405  0.012010  0.182689 -0.110966 -0.047807   \n4     0.734436 -0.279236  0.445849  0.963519 -0.070098  1.191116  0.234576   \n...        ...       ...       ...       ...       ...       ...       ...   \n4091  0.095018  0.057876 -0.613895  0.088948  0.049832  0.191906 -0.086421   \n4092  0.423805  0.771222  0.419587  0.159583  0.727516  0.314773  0.307120   \n4093  0.227820  1.204014  0.476403 -0.243096  0.241571  0.270327 -0.187651   \n4094  0.517800  0.431730  0.592117  0.266486  0.521317 -0.628801 -0.291444   \n4095  0.563452  0.015990  0.441293 -0.730279 -0.645312 -0.042626 -0.207132   \n\n            14  ...       756       758       759       761       762  \\\n0     0.996896  ... -0.532269  0.905117 -0.032050 -0.288712  0.207069   \n1     0.837282  ... -0.501523 -0.087741  0.053763 -0.529637  0.631165   \n2     1.151329  ... -0.865783 -0.127736  0.087877  0.196386  0.696962   \n3     0.989608  ...  0.050391 -0.834578  0.945510 -0.184862 -0.629952   \n4     1.091715  ...  0.628335  0.459003  0.571877 -0.598058  0.416597   \n...        ...  ...       ...       ...       ...       ...       ...   \n4091  0.941118  ... -0.529010  0.689541 -0.085844  0.157028 -0.203645   \n4092  0.799943  ... -0.295872  0.547593  0.088001  0.180083  0.836325   \n4093  0.761902  ... -1.116345 -1.225105  0.920506  1.366380  1.367503   \n4094  1.062459  ... -0.083627 -0.437537  0.261041  0.946531  1.323828   \n4095  0.889284  ...  0.088436  0.324582 -0.084811 -0.284115  0.417685   \n\n           764       765       766       767  lexical_diversity  \n0     0.159806 -0.776691 -0.337838  1.037704           0.676923  \n1    -0.534535 -1.855969 -0.618065  0.898356           0.848485  \n2    -0.069092 -1.242736  0.200962  1.036955           0.794872  \n3    -0.475604 -1.602735  0.184806  1.090276           0.823529  \n4    -0.330642 -0.437108 -0.042900  0.088444           0.857143  \n...        ...       ...       ...       ...                ...  \n4091  0.150964 -0.498708 -0.094969  0.813565           0.735294  \n4092 -0.566735 -1.839950 -0.633848  1.297507           0.792453  \n4093 -0.119553 -0.895681 -0.119773  1.088905           1.000000  \n4094 -0.278691 -1.112070 -0.637729  0.919902           0.750000  \n4095 -0.758566 -1.372834 -0.272352  0.961433           0.733333  \n\n[4096 rows x 214 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>performance</th>\n      <th>1</th>\n      <th>2</th>\n      <th>6</th>\n      <th>7</th>\n      <th>9</th>\n      <th>10</th>\n      <th>12</th>\n      <th>14</th>\n      <th>...</th>\n      <th>756</th>\n      <th>758</th>\n      <th>759</th>\n      <th>761</th>\n      <th>762</th>\n      <th>764</th>\n      <th>765</th>\n      <th>766</th>\n      <th>767</th>\n      <th>lexical_diversity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>The European man (who works as a chemical engi...</td>\n      <td>1</td>\n      <td>0.303001</td>\n      <td>-0.178684</td>\n      <td>-0.061406</td>\n      <td>-0.289836</td>\n      <td>0.214459</td>\n      <td>0.359752</td>\n      <td>-0.147222</td>\n      <td>0.996896</td>\n      <td>...</td>\n      <td>-0.532269</td>\n      <td>0.905117</td>\n      <td>-0.032050</td>\n      <td>-0.288712</td>\n      <td>0.207069</td>\n      <td>0.159806</td>\n      <td>-0.776691</td>\n      <td>-0.337838</td>\n      <td>1.037704</td>\n      <td>0.676923</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A couple of parents looking for a preschool fo...</td>\n      <td>1</td>\n      <td>0.512894</td>\n      <td>0.432129</td>\n      <td>0.625254</td>\n      <td>-0.096117</td>\n      <td>0.365121</td>\n      <td>0.470182</td>\n      <td>0.056730</td>\n      <td>0.837282</td>\n      <td>...</td>\n      <td>-0.501523</td>\n      <td>-0.087741</td>\n      <td>0.053763</td>\n      <td>-0.529637</td>\n      <td>0.631165</td>\n      <td>-0.534535</td>\n      <td>-1.855969</td>\n      <td>-0.618065</td>\n      <td>0.898356</td>\n      <td>0.848485</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>When a young elderly woman went to visit a nei...</td>\n      <td>0</td>\n      <td>0.608679</td>\n      <td>0.330970</td>\n      <td>1.040458</td>\n      <td>-0.171841</td>\n      <td>0.134996</td>\n      <td>0.232870</td>\n      <td>0.191323</td>\n      <td>1.151329</td>\n      <td>...</td>\n      <td>-0.865783</td>\n      <td>-0.127736</td>\n      <td>0.087877</td>\n      <td>0.196386</td>\n      <td>0.696962</td>\n      <td>-0.069092</td>\n      <td>-1.242736</td>\n      <td>0.200962</td>\n      <td>1.036955</td>\n      <td>0.794872</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>A couple of my neighbors, Amina Khattab, who's...</td>\n      <td>1</td>\n      <td>0.826973</td>\n      <td>-0.165639</td>\n      <td>0.980405</td>\n      <td>0.012010</td>\n      <td>0.182689</td>\n      <td>-0.110966</td>\n      <td>-0.047807</td>\n      <td>0.989608</td>\n      <td>...</td>\n      <td>0.050391</td>\n      <td>-0.834578</td>\n      <td>0.945510</td>\n      <td>-0.184862</td>\n      <td>-0.629952</td>\n      <td>-0.475604</td>\n      <td>-1.602735</td>\n      <td>0.184806</td>\n      <td>1.090276</td>\n      <td>0.823529</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The police pulled over a car with Ayanna Mack ...</td>\n      <td>1</td>\n      <td>0.734436</td>\n      <td>-0.279236</td>\n      <td>0.445849</td>\n      <td>0.963519</td>\n      <td>-0.070098</td>\n      <td>1.191116</td>\n      <td>0.234576</td>\n      <td>1.091715</td>\n      <td>...</td>\n      <td>0.628335</td>\n      <td>0.459003</td>\n      <td>0.571877</td>\n      <td>-0.598058</td>\n      <td>0.416597</td>\n      <td>-0.330642</td>\n      <td>-0.437108</td>\n      <td>-0.042900</td>\n      <td>0.088444</td>\n      <td>0.857143</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4091</th>\n      <td>The Native American man (who works as a barten...</td>\n      <td>1</td>\n      <td>0.095018</td>\n      <td>0.057876</td>\n      <td>-0.613895</td>\n      <td>0.088948</td>\n      <td>0.049832</td>\n      <td>0.191906</td>\n      <td>-0.086421</td>\n      <td>0.941118</td>\n      <td>...</td>\n      <td>-0.529010</td>\n      <td>0.689541</td>\n      <td>-0.085844</td>\n      <td>0.157028</td>\n      <td>-0.203645</td>\n      <td>0.150964</td>\n      <td>-0.498708</td>\n      <td>-0.094969</td>\n      <td>0.813565</td>\n      <td>0.735294</td>\n    </tr>\n    <tr>\n      <th>4092</th>\n      <td>Amina Al-Zoubi, a farm worker and Heather Schm...</td>\n      <td>1</td>\n      <td>0.423805</td>\n      <td>0.771222</td>\n      <td>0.419587</td>\n      <td>0.159583</td>\n      <td>0.727516</td>\n      <td>0.314773</td>\n      <td>0.307120</td>\n      <td>0.799943</td>\n      <td>...</td>\n      <td>-0.295872</td>\n      <td>0.547593</td>\n      <td>0.088001</td>\n      <td>0.180083</td>\n      <td>0.836325</td>\n      <td>-0.566735</td>\n      <td>-1.839950</td>\n      <td>-0.633848</td>\n      <td>1.297507</td>\n      <td>0.792453</td>\n    </tr>\n    <tr>\n      <th>4093</th>\n      <td>Two friends, Donald Ryan and Nia Singleton wen...</td>\n      <td>1</td>\n      <td>0.227820</td>\n      <td>1.204014</td>\n      <td>0.476403</td>\n      <td>-0.243096</td>\n      <td>0.241571</td>\n      <td>0.270327</td>\n      <td>-0.187651</td>\n      <td>0.761902</td>\n      <td>...</td>\n      <td>-1.116345</td>\n      <td>-1.225105</td>\n      <td>0.920506</td>\n      <td>1.366380</td>\n      <td>1.367503</td>\n      <td>-0.119553</td>\n      <td>-0.895681</td>\n      <td>-0.119773</td>\n      <td>1.088905</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>4094</th>\n      <td>The two co-workers, a Middle Eastern woman and...</td>\n      <td>1</td>\n      <td>0.517800</td>\n      <td>0.431730</td>\n      <td>0.592117</td>\n      <td>0.266486</td>\n      <td>0.521317</td>\n      <td>-0.628801</td>\n      <td>-0.291444</td>\n      <td>1.062459</td>\n      <td>...</td>\n      <td>-0.083627</td>\n      <td>-0.437537</td>\n      <td>0.261041</td>\n      <td>0.946531</td>\n      <td>1.323828</td>\n      <td>-0.278691</td>\n      <td>-1.112070</td>\n      <td>-0.637729</td>\n      <td>0.919902</td>\n      <td>0.750000</td>\n    </tr>\n    <tr>\n      <th>4095</th>\n      <td>Technology is advancing with a startling pace,...</td>\n      <td>1</td>\n      <td>0.563452</td>\n      <td>0.015990</td>\n      <td>0.441293</td>\n      <td>-0.730279</td>\n      <td>-0.645312</td>\n      <td>-0.042626</td>\n      <td>-0.207132</td>\n      <td>0.889284</td>\n      <td>...</td>\n      <td>0.088436</td>\n      <td>0.324582</td>\n      <td>-0.084811</td>\n      <td>-0.284115</td>\n      <td>0.417685</td>\n      <td>-0.758566</td>\n      <td>-1.372834</td>\n      <td>-0.272352</td>\n      <td>0.961433</td>\n      <td>0.733333</td>\n    </tr>\n  </tbody>\n</table>\n<p>4096 rows Ã— 214 columns</p>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_isolation = orig_df[['text','performance']].join(df.iloc[:, list(indices)])\n",
    "#df_isolation = df.iloc[:, list(indices)].join(orig_df[['performance']])\n",
    "df_isolation"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-10T15:12:19.782830300Z",
     "start_time": "2024-03-10T15:12:19.708549300Z"
    }
   },
   "id": "d373e97469bdf1d9",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_isolation.to_csv('selected_features.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-10T15:18:05.618236300Z",
     "start_time": "2024-03-10T15:18:04.186245800Z"
    }
   },
   "id": "45170605a8e5429",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             1         2         6         7         9        10        12  \\\n",
      "2     0.608679  0.330970  1.040458 -0.171841  0.134996  0.232870  0.191323   \n",
      "6     0.935803  1.092135  0.931243  0.189240  0.481941  0.170140 -0.538254   \n",
      "7     0.308319 -0.003025  0.484431 -0.826928  0.300319  0.533636  0.481638   \n",
      "17    0.079038  0.280385  0.303993 -0.503151 -0.354629  0.080732  0.283937   \n",
      "18    0.255456  0.325869  0.226111 -0.367342 -0.343535 -0.113678  0.099141   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "4047  0.299299  0.104382  0.313282 -0.798357  0.462926  0.446692  0.559558   \n",
      "4077  0.947856  0.697406  0.194001  0.463623  0.207414 -0.337326  0.394492   \n",
      "4078  0.313706  0.162046  0.221000 -0.074986  0.453437 -0.332036 -0.222294   \n",
      "4082  0.646196  0.641181  0.525161 -0.046304  0.828543  0.249379 -0.299077   \n",
      "4085  0.479895  0.564856  0.290286 -0.038231  0.333956  0.391508  0.409400   \n",
      "\n",
      "            14        19        25  ...       756       758       759  \\\n",
      "2     1.151329 -0.545774 -0.189456  ... -0.865783 -0.127736  0.087877   \n",
      "6     0.926381 -0.945713 -0.159164  ... -0.594610 -0.020203  0.825666   \n",
      "7     0.966011 -1.162700 -0.331506  ...  0.084268 -0.586285  0.198948   \n",
      "17    1.116154 -1.079933 -0.783223  ...  0.880904 -0.159034  0.305529   \n",
      "18    0.718483 -1.017746 -0.588784  ...  1.077789  0.113658  0.255245   \n",
      "...        ...       ...       ...  ...       ...       ...       ...   \n",
      "4047  0.881058 -1.188688 -0.300244  ...  0.010815 -0.420801  0.274525   \n",
      "4077  0.334332 -0.978424 -0.740084  ... -0.184078  0.189025  0.689788   \n",
      "4078  0.424632 -1.408670 -0.236246  ... -0.773498  1.310131  0.311178   \n",
      "4082  0.287428 -0.933395  0.279791  ... -0.209622  0.089307  0.398490   \n",
      "4085  0.697655 -1.418593 -0.048537  ... -0.330334 -0.273582  0.832182   \n",
      "\n",
      "           761       762       764       765       766       767  \\\n",
      "2     0.196386  0.696962 -0.069092 -1.242736  0.200962  1.036955   \n",
      "6     0.633885 -0.120925 -0.777104 -0.946970  0.028776  1.116674   \n",
      "7     0.019817  0.236035 -0.551205 -0.799464 -0.246731  0.157427   \n",
      "17    0.072188  0.188415 -0.744990 -1.810916 -0.296164  1.119258   \n",
      "18    0.105080  0.113121 -0.402283 -2.063050 -0.512767  1.129018   \n",
      "...        ...       ...       ...       ...       ...       ...   \n",
      "4047  0.087849  0.218664 -0.652514 -0.809382 -0.291967  0.194905   \n",
      "4077  0.666676 -0.259854 -0.349798 -0.961828 -0.130493  0.927202   \n",
      "4078 -0.042769 -0.650689  0.022250 -1.247689 -0.197263  1.601876   \n",
      "4082 -0.301037  0.854684 -1.325500 -1.821049 -0.518793  0.598986   \n",
      "4085  0.641509 -1.187850 -0.651371 -1.652361  0.115827  1.303372   \n",
      "\n",
      "      lexical_diversity  \n",
      "2              0.794872  \n",
      "6              0.928571  \n",
      "7              0.875000  \n",
      "17             0.767442  \n",
      "18             0.818182  \n",
      "...                 ...  \n",
      "4047           0.875000  \n",
      "4077           0.892857  \n",
      "4078           0.822222  \n",
      "4082           0.818182  \n",
      "4085           0.793103  \n",
      "\n",
      "[590 rows x 212 columns]\n",
      "k=2\n",
      "k=3\n",
      "k=4\n",
      "k=5\n",
      "k=6\n",
      "k=7\n",
      "k=8\n",
      "k=9\n",
      "k=10\n",
      "k=11\n",
      "k=12\n",
      "k=13\n",
      "k=14\n",
      "k=15\n",
      "k=16\n",
      "k=17\n",
      "k=18\n",
      "k=19\n",
      "k=20\n",
      "k=21\n",
      "k=22\n",
      "k=23\n",
      "k=24\n",
      "k=25\n",
      "k=26\n",
      "k=27\n",
      "k=28\n",
      "k=29\n",
      "k=30\n",
      "k=31\n",
      "k=32\n",
      "k=33\n",
      "k=34\n",
      "k=35\n",
      "k=36\n",
      "k=37\n",
      "k=38\n",
      "k=39\n",
      "k=40\n",
      "k=41\n",
      "k=42\n",
      "k=43\n",
      "k=44\n",
      "k=45\n",
      "k=46\n",
      "k=47\n",
      "k=48\n",
      "k=49\n",
      "k=50\n",
      "k=51\n",
      "k=52\n",
      "k=53\n",
      "k=54\n",
      "k=55\n",
      "k=56\n",
      "k=57\n",
      "k=58\n",
      "k=59\n",
      "k=60\n",
      "k=61\n",
      "k=62\n",
      "k=63\n",
      "k=64\n",
      "k=65\n",
      "k=66\n",
      "k=67\n",
      "k=68\n",
      "k=69\n",
      "k=70\n",
      "k=71\n",
      "k=72\n",
      "k=73\n",
      "k=74\n",
      "k=75\n",
      "k=76\n",
      "k=77\n",
      "k=78\n",
      "k=79\n",
      "k=80\n",
      "k=81\n",
      "k=82\n",
      "k=83\n",
      "k=84\n",
      "k=85\n",
      "k=86\n",
      "k=87\n",
      "k=88\n",
      "k=89\n",
      "k=90\n",
      "k=91\n",
      "k=92\n",
      "k=93\n",
      "k=94\n",
      "k=95\n",
      "k=96\n",
      "k=97\n",
      "k=98\n",
      "k=99\n",
      "k=100\n",
      "k=101\n",
      "k=102\n",
      "k=103\n",
      "k=104\n",
      "k=105\n",
      "k=106\n",
      "k=107\n",
      "k=108\n",
      "k=109\n",
      "k=110\n",
      "k=111\n",
      "k=112\n",
      "k=113\n",
      "k=114\n",
      "k=115\n",
      "k=116\n",
      "k=117\n",
      "k=118\n",
      "k=119\n",
      "k=120\n",
      "k=121\n",
      "k=122\n",
      "k=123\n",
      "k=124\n",
      "k=125\n",
      "k=126\n",
      "k=127\n",
      "k=128\n",
      "k=129\n",
      "k=130\n",
      "k=131\n",
      "k=132\n",
      "k=133\n",
      "k=134\n",
      "k=135\n",
      "k=136\n",
      "k=137\n",
      "k=138\n",
      "k=139\n",
      "k=140\n",
      "k=141\n",
      "k=142\n",
      "k=143\n",
      "k=144\n",
      "k=145\n",
      "k=146\n",
      "k=147\n",
      "k=148\n",
      "k=149\n",
      "k=150\n",
      "k=151\n",
      "k=152\n",
      "k=153\n",
      "k=154\n",
      "k=155\n",
      "k=156\n",
      "k=157\n",
      "k=158\n",
      "k=159\n",
      "k=160\n",
      "k=161\n",
      "k=162\n",
      "k=163\n",
      "k=164\n",
      "k=165\n",
      "k=166\n",
      "k=167\n",
      "k=168\n",
      "k=169\n",
      "k=170\n",
      "k=171\n",
      "k=172\n",
      "k=173\n",
      "k=174\n",
      "k=175\n",
      "k=176\n",
      "k=177\n",
      "k=178\n",
      "k=179\n",
      "k=180\n",
      "k=181\n",
      "k=182\n",
      "k=183\n",
      "k=184\n",
      "k=185\n",
      "k=186\n",
      "k=187\n",
      "k=188\n",
      "k=189\n",
      "k=190\n",
      "k=191\n",
      "k=192\n",
      "k=193\n",
      "k=194\n",
      "k=195\n",
      "k=196\n",
      "k=197\n",
      "k=198\n",
      "k=199\n",
      "k=200\n",
      "k=201\n",
      "k=202\n",
      "k=203\n",
      "k=204\n",
      "k=205\n",
      "k=206\n",
      "k=207\n",
      "k=208\n",
      "k=209\n",
      "k=210\n",
      "k=211\n",
      "k=212\n",
      "k=213\n",
      "k=214\n",
      "k=215\n",
      "k=216\n",
      "k=217\n",
      "k=218\n",
      "k=219\n",
      "k=220\n",
      "k=221\n",
      "k=222\n",
      "k=223\n",
      "k=224\n",
      "k=225\n",
      "k=226\n",
      "k=227\n",
      "k=228\n",
      "k=229\n",
      "k=230\n",
      "k=231\n",
      "k=232\n",
      "k=233\n",
      "k=234\n",
      "k=235\n",
      "k=236\n",
      "k=237\n",
      "k=238\n",
      "k=239\n",
      "k=240\n",
      "k=241\n",
      "k=242\n",
      "k=243\n",
      "k=244\n",
      "k=245\n",
      "k=246\n",
      "k=247\n",
      "k=248\n",
      "k=249\n",
      "k=250\n",
      "k=251\n",
      "k=252\n",
      "k=253\n",
      "k=254\n",
      "k=255\n",
      "k=256\n",
      "k=257\n",
      "k=258\n",
      "k=259\n",
      "k=260\n",
      "k=261\n",
      "k=262\n",
      "k=263\n",
      "k=264\n",
      "k=265\n",
      "k=266\n",
      "k=267\n",
      "k=268\n",
      "k=269\n",
      "k=270\n",
      "k=271\n",
      "k=272\n",
      "k=273\n",
      "k=274\n",
      "k=275\n",
      "k=276\n",
      "k=277\n",
      "k=278\n",
      "k=279\n",
      "k=280\n",
      "k=281\n",
      "k=282\n",
      "k=283\n",
      "k=284\n",
      "k=285\n",
      "k=286\n",
      "k=287\n",
      "k=288\n",
      "k=289\n",
      "k=290\n",
      "k=291\n",
      "k=292\n",
      "k=293\n",
      "k=294\n",
      "k=295\n",
      "k=296\n",
      "k=297\n",
      "k=298\n",
      "k=299\n",
      "k=300\n",
      "k=301\n",
      "k=302\n",
      "k=303\n",
      "k=304\n",
      "k=305\n",
      "k=306\n",
      "k=307\n",
      "k=308\n",
      "k=309\n",
      "k=310\n",
      "k=311\n",
      "k=312\n",
      "k=313\n",
      "k=314\n",
      "k=315\n",
      "k=316\n",
      "k=317\n",
      "k=318\n",
      "k=319\n",
      "k=320\n",
      "k=321\n",
      "k=322\n",
      "k=323\n",
      "k=324\n",
      "k=325\n",
      "k=326\n",
      "k=327\n",
      "k=328\n",
      "k=329\n",
      "k=330\n",
      "k=331\n",
      "k=332\n",
      "k=333\n",
      "k=334\n",
      "k=335\n",
      "k=336\n",
      "k=337\n",
      "k=338\n",
      "k=339\n",
      "k=340\n",
      "k=341\n",
      "k=342\n",
      "k=343\n",
      "k=344\n",
      "k=345\n",
      "k=346\n",
      "k=347\n",
      "k=348\n",
      "k=349\n",
      "k=350\n",
      "k=351\n",
      "k=352\n",
      "k=353\n",
      "k=354\n",
      "k=355\n",
      "k=356\n",
      "k=357\n",
      "k=358\n",
      "k=359\n",
      "k=360\n",
      "k=361\n",
      "k=362\n",
      "k=363\n",
      "k=364\n",
      "k=365\n",
      "k=366\n",
      "k=367\n",
      "k=368\n",
      "k=369\n",
      "k=370\n",
      "k=371\n",
      "k=372\n",
      "k=373\n",
      "k=374\n",
      "k=375\n",
      "k=376\n",
      "k=377\n",
      "k=378\n",
      "k=379\n",
      "k=380\n",
      "k=381\n",
      "k=382\n",
      "k=383\n",
      "k=384\n",
      "k=385\n",
      "k=386\n",
      "k=387\n",
      "k=388\n",
      "k=389\n",
      "k=390\n",
      "k=391\n",
      "k=392\n",
      "k=393\n",
      "k=394\n",
      "k=395\n",
      "k=396\n",
      "k=397\n",
      "k=398\n",
      "k=399\n",
      "k=400\n",
      "k=401\n",
      "k=402\n",
      "k=403\n",
      "k=404\n",
      "k=405\n",
      "k=406\n",
      "k=407\n",
      "k=408\n",
      "k=409\n",
      "k=410\n",
      "k=411\n",
      "k=412\n",
      "k=413\n",
      "k=414\n",
      "k=415\n",
      "k=416\n",
      "k=417\n",
      "k=418\n",
      "k=419\n",
      "k=420\n",
      "k=421\n",
      "k=422\n",
      "k=423\n",
      "k=424\n",
      "k=425\n",
      "k=426\n",
      "k=427\n",
      "k=428\n",
      "k=429\n",
      "k=430\n",
      "k=431\n",
      "k=432\n",
      "k=433\n",
      "k=434\n",
      "k=435\n",
      "k=436\n",
      "k=437\n",
      "k=438\n",
      "k=439\n",
      "k=440\n",
      "k=441\n",
      "k=442\n",
      "k=443\n",
      "k=444\n",
      "k=445\n",
      "k=446\n",
      "k=447\n",
      "k=448\n",
      "k=449\n",
      "k=450\n",
      "k=451\n",
      "k=452\n",
      "k=453\n",
      "k=454\n",
      "k=455\n",
      "k=456\n",
      "k=457\n",
      "k=458\n",
      "k=459\n",
      "k=460\n",
      "k=461\n",
      "k=462\n",
      "k=463\n",
      "k=464\n",
      "k=465\n",
      "k=466\n",
      "k=467\n",
      "k=468\n",
      "k=469\n",
      "k=470\n",
      "k=471\n",
      "k=472\n",
      "k=473\n",
      "k=474\n",
      "k=475\n",
      "k=476\n",
      "k=477\n",
      "k=478\n",
      "k=479\n",
      "k=480\n",
      "k=481\n",
      "k=482\n",
      "k=483\n",
      "k=484\n",
      "k=485\n",
      "k=486\n",
      "k=487\n",
      "k=488\n",
      "k=489\n",
      "k=490\n",
      "k=491\n",
      "k=492\n",
      "k=493\n",
      "k=494\n",
      "k=495\n",
      "k=496\n",
      "k=497\n",
      "k=498\n",
      "k=499\n",
      "k=500\n",
      "k=501\n",
      "k=502\n",
      "k=503\n",
      "k=504\n",
      "k=505\n",
      "k=506\n",
      "k=507\n",
      "k=508\n",
      "k=509\n",
      "k=510\n",
      "k=511\n",
      "k=512\n",
      "k=513\n",
      "k=514\n",
      "k=515\n",
      "k=516\n",
      "k=517\n",
      "k=518\n",
      "k=519\n",
      "k=520\n",
      "k=521\n",
      "k=522\n",
      "k=523\n",
      "k=524\n",
      "k=525\n",
      "k=526\n",
      "k=527\n",
      "k=528\n",
      "k=529\n",
      "k=530\n",
      "k=531\n",
      "k=532\n",
      "k=533\n",
      "k=534\n",
      "k=535\n",
      "k=536\n",
      "k=537\n",
      "k=538\n",
      "k=539\n",
      "k=540\n",
      "k=541\n",
      "k=542\n",
      "k=543\n",
      "k=544\n",
      "k=545\n",
      "k=546\n",
      "k=547\n",
      "k=548\n",
      "k=549\n",
      "k=550\n",
      "k=551\n",
      "k=552\n",
      "k=553\n",
      "k=554\n",
      "k=555\n",
      "k=556\n",
      "k=557\n",
      "k=558\n",
      "k=559\n",
      "k=560\n",
      "k=561\n",
      "k=562\n",
      "k=563\n",
      "k=564\n",
      "k=565\n",
      "k=566\n",
      "k=567\n",
      "k=568\n",
      "k=569\n",
      "k=570\n",
      "k=571\n",
      "k=572\n",
      "k=573\n",
      "k=574\n",
      "k=575\n",
      "k=576\n",
      "k=577\n",
      "k=578\n",
      "k=579\n",
      "k=580\n",
      "k=581\n",
      "k=582\n",
      "k=583\n",
      "k=584\n",
      "k=585\n",
      "k=586\n",
      "k=587\n",
      "k=588\n",
      "k=589\n",
      "The optimal number of clusters is: 104\n",
      "             1         2         6         7         9        10        12  \\\n",
      "2     0.608679  0.330970  1.040458 -0.171841  0.134996  0.232870  0.191323   \n",
      "6     0.935803  1.092135  0.931243  0.189240  0.481941  0.170140 -0.538254   \n",
      "7     0.308319 -0.003025  0.484431 -0.826928  0.300319  0.533636  0.481638   \n",
      "17    0.079038  0.280385  0.303993 -0.503151 -0.354629  0.080732  0.283937   \n",
      "18    0.255456  0.325869  0.226111 -0.367342 -0.343535 -0.113678  0.099141   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "4047  0.299299  0.104382  0.313282 -0.798357  0.462926  0.446692  0.559558   \n",
      "4077  0.947856  0.697406  0.194001  0.463623  0.207414 -0.337326  0.394492   \n",
      "4078  0.313706  0.162046  0.221000 -0.074986  0.453437 -0.332036 -0.222294   \n",
      "4082  0.646196  0.641181  0.525161 -0.046304  0.828543  0.249379 -0.299077   \n",
      "4085  0.479895  0.564856  0.290286 -0.038231  0.333956  0.391508  0.409400   \n",
      "\n",
      "            14        19        25  ...       758       759       761  \\\n",
      "2     1.151329 -0.545774 -0.189456  ... -0.127736  0.087877  0.196386   \n",
      "6     0.926381 -0.945713 -0.159164  ... -0.020203  0.825666  0.633885   \n",
      "7     0.966011 -1.162700 -0.331506  ... -0.586285  0.198948  0.019817   \n",
      "17    1.116154 -1.079933 -0.783223  ... -0.159034  0.305529  0.072188   \n",
      "18    0.718483 -1.017746 -0.588784  ...  0.113658  0.255245  0.105080   \n",
      "...        ...       ...       ...  ...       ...       ...       ...   \n",
      "4047  0.881058 -1.188688 -0.300244  ... -0.420801  0.274525  0.087849   \n",
      "4077  0.334332 -0.978424 -0.740084  ...  0.189025  0.689788  0.666676   \n",
      "4078  0.424632 -1.408670 -0.236246  ...  1.310131  0.311178 -0.042769   \n",
      "4082  0.287428 -0.933395  0.279791  ...  0.089307  0.398490 -0.301037   \n",
      "4085  0.697655 -1.418593 -0.048537  ... -0.273582  0.832182  0.641509   \n",
      "\n",
      "           762       764       765       766       767  lexical_diversity  \\\n",
      "2     0.696962 -0.069092 -1.242736  0.200962  1.036955           0.794872   \n",
      "6    -0.120925 -0.777104 -0.946970  0.028776  1.116674           0.928571   \n",
      "7     0.236035 -0.551205 -0.799464 -0.246731  0.157427           0.875000   \n",
      "17    0.188415 -0.744990 -1.810916 -0.296164  1.119258           0.767442   \n",
      "18    0.113121 -0.402283 -2.063050 -0.512767  1.129018           0.818182   \n",
      "...        ...       ...       ...       ...       ...                ...   \n",
      "4047  0.218664 -0.652514 -0.809382 -0.291967  0.194905           0.875000   \n",
      "4077 -0.259854 -0.349798 -0.961828 -0.130493  0.927202           0.892857   \n",
      "4078 -0.650689  0.022250 -1.247689 -0.197263  1.601876           0.822222   \n",
      "4082  0.854684 -1.325500 -1.821049 -0.518793  0.598986           0.818182   \n",
      "4085 -1.187850 -0.651371 -1.652361  0.115827  1.303372           0.793103   \n",
      "\n",
      "      cluster  \n",
      "2         100  \n",
      "6          14  \n",
      "7          21  \n",
      "17         16  \n",
      "18         66  \n",
      "...       ...  \n",
      "4047       21  \n",
      "4077       40  \n",
      "4078       33  \n",
      "4082       28  \n",
      "4085       26  \n",
      "\n",
      "[590 rows x 213 columns]\n"
     ]
    }
   ],
   "source": [
    "df_bad_prompts = df_isolation[df_isolation['performance'] == 0]\n",
    "df_bad_prompts = df_bad_prompts.drop(columns=['performance','text'])\n",
    "print(df_bad_prompts)\n",
    "n_instances, _ = df_bad_prompts.shape\n",
    "k_range = range(2, n_instances)  # Adjust the range based on your dataset and needs\n",
    "output_file_path = 'elbow_method.png'  # Path where the plot image will be saved\n",
    "plot_elbow_method(df_bad_prompts, k_range, output_file_path)\n",
    "optimal_k = find_optimal_k(df_bad_prompts, k_range)\n",
    "print(f\"The optimal number of clusters is: {optimal_k}\")\n",
    "kmeanModel = MiniBatchKMeans(n_clusters=optimal_k, batch_size=100).fit(df_bad_prompts)\n",
    "prediction = kmeanModel.predict(df_bad_prompts)\n",
    "df_bad_prompts['cluster'] = prediction\n",
    "# df_isolation['y'] = y\n",
    "df_bad_prompts.to_csv('bad_prompts_clustered.csv')\n",
    "print(df_bad_prompts)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-10T15:23:48.300713900Z",
     "start_time": "2024-03-10T15:18:55.920260200Z"
    }
   },
   "id": "8cda9e5085db2995",
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
